{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCJsonData:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.json_data = self._load_json()\n",
    "\n",
    "    def _load_json(self):\n",
    "        try:\n",
    "            f = open(self.file_path, \"r\")\n",
    "        except FileNotFoundError:\n",
    "            f = open(self.file_path, \"w\")\n",
    "            f.write(\"{\\\"posts\\\":[]}\")\n",
    "            f.close()\n",
    "            f = open(self.file_path, \"r\")\n",
    "        return json.load(f)\n",
    "\n",
    "    def save2txt(self):\n",
    "        txt_path = '.'.join(self.file_path.split('.')[:-1] + ['txt'])\n",
    "        with open(txt_path, 'w') as f:\n",
    "            for postid in self.json_data.keys():\n",
    "                post = self.json_data[postid]\n",
    "                for tag in [\"title\", \"content\"]:\n",
    "                    txt = post[tag]\n",
    "                    if txt == '': continue\n",
    "                    f.write(txt+'\\n')\n",
    "                for comment in post[\"comments\"]:\n",
    "                    f.write(comment[\"content\"]+'\\n')\n",
    "                    if txt == '': continue\n",
    "            f.close()\n",
    "\n",
    "    def save(self, results):\n",
    "        # json_data = {}\n",
    "        # try:\n",
    "        #     f = open(file_path, \"r\")\n",
    "        # except FileNotFoundError:\n",
    "        #     f = open(file_path, \"w\")\n",
    "        #     f.write(\"{\\\"posts\\\":[]}\")\n",
    "        #     f.close()\n",
    "        #     f = open(file_path, \"r\")\n",
    "\n",
    "        # # with open(file_path, \"r\") as json_file:\n",
    "        # json_data = json.load(f)\n",
    "\n",
    "        for result in results:\n",
    "            self.json_data['posts'].append(result)\n",
    "        with open(self.file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(self.json_data, outfile, indent=4, ensure_ascii=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "item = json.load(open(\"dcinside_crawling.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcjson = DCJsonData('dcinside_crawling.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcjson.save2txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코퍼스전처리\n",
    "* rerefenced from : kcbert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip install soynlp emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x):\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/private/eta-crawler/src\n"
     ]
    }
   ],
   "source": [
    "cd /data/private/eta-crawler/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_count = 0\n",
    "for idx in range(7,8):\n",
    "    # !sed -i '/^$/d' '{idx}.txt'\n",
    "    with open(f'{idx}.txt', 'r') as f:\n",
    "        with open(f'normalized/{idx}.txt', 'w') as wf:\n",
    "            while True:\n",
    "                txt = f.readline().strip()\n",
    "                if txt == '': \n",
    "                    retry_count+=1\n",
    "                    if retry_count >= 2: break\n",
    "                    continue\n",
    "                retry_count=0\n",
    "                txt = clean(txt)\n",
    "                wf.write(txt+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korpora (KcBERT) 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, http://kakaobrain-pypi.dev.9rum.cc/\n",
      "Requirement already satisfied: Korpora in /opt/conda/lib/python3.7/site-packages (0.1.1)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in /opt/conda/lib/python3.7/site-packages (from Korpora) (4.50.2)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from Korpora) (2.23.0)\n",
      "Requirement already satisfied: dataclasses>=0.6 in /opt/conda/lib/python3.7/site-packages (from Korpora) (0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->Korpora) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->Korpora) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->Korpora) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->Korpora) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[kcbert] download kcbert-train.tar.gzaa: 100%|██████████| 2.10G/2.10G [02:42<00:00, 12.9MB/s]    \n",
      "[kcbert] download kcbert-train.tar.gzab: 100%|██████████| 2.10G/2.10G [02:42<00:00, 12.9MB/s]    \n",
      "[kcbert] download kcbert-train.tar.gzac: 671MB [00:54, 12.3MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip tar. It needs a few minutes ... done\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch('kcbert', root_dir='/data/private/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Tokenizer 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraConfig, ElectraModel, ElectraTokenizerFast\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# paths = [str(x) for x in Path(\"./eo_data/\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Must be False if cased model\n",
    "    lowercase=False\n",
    ")\n",
    "tokenizer.train(['kcbert/tokenizer_train.txt'], vocab_size=30000, limit_alphabet=4000)\n",
    "tokenizer.save_model('kcbert', '30000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /data/private/eta-crawler/src/normalized/*.txt /data/private/kcbert/kcbert.small.txt \\\n",
    "#     > /data/private/kcbert/tokenizer_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Custom Config\n",
    "config를 vocab사이즈를 맞추어 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "size='small' # base, large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c858b9fbac404143b1bc4d6294ca0f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdf55ae4cde4cea8c77ca61adf9fe3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=463.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraConfig\n",
    "\n",
    "disc = ElectraConfig.from_pretrained(f\"google/electra-{size}-discriminator\")\n",
    "gen = ElectraConfig.from_pretrained(f\"google/electra-{size}-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.vocab_size = 16000\n",
    "gen.vocab_size = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraConfig {\n",
       "  \"architectures\": [\n",
       "    \"ElectraForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"electra\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"summary_activation\": \"gelu\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"first\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 16000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraConfig {\n",
       "  \"architectures\": [\n",
       "    \"ElectraForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"electra\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"summary_activation\": \"gelu\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"first\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 16000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.save_pretrained('soongsil-small-discriminator')\n",
    "gen.save_pretrained('soongsil-small-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
